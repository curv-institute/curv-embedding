# Claude Experiment Prompt — v1.6.0 Cost-Weighted Mutation Metrics + v1.5.0 Takeaway Logging

You are Claude Code, acting as a senior ML systems engineer and research engineer.

This prompt runs the v1.6.0 measurement refinement cycle for curv-embedding. It introduces cost-weighted mutation metrics to correctly interpret the v1.5.0 hybrid results, without changing any chunking or embedding behavior.

This is a measurement-only cycle.

Must comply with CLAUDE.md, AGENTS.md, AGENT archival, jj/git discipline, and minor version auto-tagging.

---

## Canonical Start Prompt (Authoritative)

Context: Continue the curv-embedding project.
- v1.2.0 established structural benefits of stability-driven chunking.
- v1.3.0 showed streaming parity.
- v1.4.0 revealed a negative result: stability-driven chunking is more mutation-sensitive.
- v1.5.0 introduced a hybrid policy that localized change (80.7% localization efficiency) but appeared worse under raw re-embed fraction.

Goal: Replace misleading count-based mutation metrics with cost-weighted metrics that reflect actual update impact, and re-evaluate baseline vs stability vs hybrid chunking without changing algorithms.

Hypothesis: When mutation cost is measured in bytes (or weighted mass) rather than chunk count, hybrid chunking will show lower effective mutation cost than fixed-size baseline, consistent with its high localization efficiency.

Constraints: deterministic, reproducible, no changes to chunking, embedding, or FAISS logic; metrics and reporting only.

Deliverables: new metrics, updated reports/figures, paper-ready tables, manifests, and a clean v1.6.0 tag.

Non-goals: algorithm tuning, parameter sweeps, reranking, retrieval evaluation.

---

## Required Notes to Log About v1.5.0 (Mandatory)

Before computing new metrics, log the following interpretive note verbatim in:
- AGENT/<T>-out.md for this cycle, and
- the CHANGELOG.md entry for v1.6.0.

**v1.5.0 Takeaway:** Hybrid chunking increased raw re-embed fraction because localized micro-chunking converts few large invalidations into many small ones. Raw chunk-count metrics therefore overstate mutation cost. Localization efficiency (80.7%) indicates that hybrid chunking confines change impact, motivating cost-weighted mutation metrics.

---

## New Metrics (Authoritative)

Implement the following metrics on top of existing outputs.

### A) Byte-weighted re-embed cost (primary)

For each update step:

```
reembed_bytes = sum(bytes(c) for c in reembedded_chunks)
reembed_fraction_bytes = reembed_bytes / total_bytes
```

Report:
- per-step values
- cumulative values
- mean / p90 across runs

This is the primary mutation metric going forward.

---

### B) Stability-weighted re-embed cost (secondary)

Approximate semantic mass using existing diagnostics:

```
reembed_weighted = sum(bytes(c) * stability_score(c))
reembed_fraction_weighted = reembed_weighted / sum(bytes * stability_score)
```

Where stability_score(c) is normalized from stored stability diagnostics.

This metric is exploratory and must be clearly labeled as such.

---

### C) Cost-localization curve

For each update:
- sort re-embedded chunks by distance from edit window
- plot cumulative reembed_bytes vs distance

This visualizes how quickly mutation cost decays away from edits.

---

## Re-analysis Plan

Recompute metrics for the existing v1.5.0 runs:
- baseline
- stability-driven
- hybrid

Do NOT rerun experiments unless strictly necessary; prefer post-hoc analysis from stored manifests and SQLite.

---

## Required Outputs

Generate:
- paper/tables/temporal_cost_weighted_v1.5.0.tex
- paper/figures/temporal_cost_weighted_v1.5.0.pdf
- paper/figures/cost_localization_curves_v1.5.0.pdf

Update report summaries to:
- clearly distinguish count-based vs cost-based mutation metrics

---

## Execution Plan (Parallelize)

- Subagent A: metric definitions + SQLite queries
- Subagent B: post-hoc re-analysis scripts
- Subagent C: plotting (cost curves, comparisons)
- Subagent D: paper tables + captions + wording updates

---

## Steps (Do in Order)

### 0) AGENT archive (mandatory)
- Compute UNIX time <T>.
- Save this prompt to AGENT/<T>-in.md.

### 1) Checkout latest tag

```bash
git checkout v1.5.0
uv venv .venv
source .venv/bin/activate
```

### 2) Run re-analysis

Use existing results under:
- eval/results/v1.4.0-*-temporal-*
- eval/results/v1.5.0-*-temporal-*

Compute new metrics without modifying original artifacts.

---

### 3) Generate reports and figures

Write new artifacts under paper/ as specified.

---

### 4) Record execution output

Write a concise summary to AGENT/<T>-out.md including:
- comparison of count-based vs byte-weighted metrics
- whether hypothesis is supported

Commit and push AGENT files.

---

## Post-cycle release: v1.6.0 (mandatory)

After metrics and artifacts are complete:
- Update VERSION → 1.6.0
- Add CHANGELOG.md entry:

```markdown
## v1.6.0
- Add cost-weighted mutation metrics (byte-weighted, stability-weighted)
- Reinterpret hybrid chunking results using localization-aware cost
- Log v1.5.0 takeaway motivating metric correction
- AGENT prompt used: `AGENT/<T>-in.md`
```

- Commit via jj, push, tag annotated v1.6.0, push tags.

---

## Stop Condition

Stop when:
- cost-weighted metrics computed
- figures and tables generated
- AGENT archive committed and pushed
- v1.6.0 tagged and pushed

Do not change chunking or embedding behavior in this cycle.
