# Claude Experiment Prompt â€” v1.1.0 Matrix (Baseline vs Stability Chunking, Offline+Streaming)

You are Claude Code, acting as a senior ML systems engineer and research engineer.

This prompt runs the first full experiment matrix for curv-embedding at tag v1.1.0, generates paper-ready artifacts, archives prompts/outputs in AGENT/, and then cuts the next minor tag v1.2.0 per contract.

This must comply with CLAUDE.md, AGENTS.md, and the Canonical Prompt Contract.

---

## Context

Repo: https://github.com/curv-institute/curv-embedding
Current tagged release: v1.1.0
Diagnostics mode: proxy_entropy (must be recorded in manifests)
Vector engine: FAISS + SQLite (WAL enabled, checkpointed; events table)

---

## Goal

Run a minimal, reproducible experiment matrix comparing:
1. Heuristic baseline chunking
2. Stability-driven chunking (offline)
3. Stability-driven chunking (streaming)

and quantify:
- embedding drift
- ANN neighbor churn
- top-k overlap (query reformulation)
- boundary sensitivity
- maintenance cost

---

## Hypothesis

Stability-driven chunking reduces drift and neighbor churn, and improves top-k overlap under updates and reformulation, particularly near boundaries, at the cost of some overhead.

---

## Constraints

- Deterministic, reproducible
- Do not change algorithmic code during this prompt
- Use per-run result isolation: eval/results/<run_name>/
- Use run naming convention including tag

---

## Required Runs (Name exactly like this)

Use today's date as YYYYMMDD.

1. Baseline (heuristic):
   - v1.1.0-baseline-offline-YYYYMMDD

2. Stability cut-score (offline):
   - v1.1.0-stability-offline-YYYYMMDD

3. Stability cut-score (streaming):
   - v1.1.0-stability-streaming-YYYYMMDD

---

## Execution Plan (Parallelize)

Use parallel subagents:
- Subagent A: Data generation + manifests
- Subagent B: Run 1 + validate outputs
- Subagent C: Run 2 + validate outputs
- Subagent D: Run 3 + validate outputs
- Subagent E: Aggregate metrics + plots + reports

No code changes are expected; subagents should only run scripts and validate outputs.

---

## Steps (Do in Order)

### 0) AGENT archive (mandatory)

- Compute UNIX time <T>.
- Save this prompt to AGENT/<T>-in.md.

### 1) Checkout tag

```bash
git checkout v1.1.0
uv venv .venv
source .venv/bin/activate
```

### 2) Generate evaluation data

```bash
uv run scripts/make_data.py --output-dir eval/data
```

Confirm manifests include:
- boundary offsets
- query families
- planted anchors (if implemented)

### 3) Run experiments

Run each condition with identical embedding model/version:

```bash
uv run scripts/run_experiment.py --run-name v1.1.0-baseline-offline-YYYYMMDD
uv run scripts/run_experiment.py --run-name v1.1.0-stability-offline-YYYYMMDD
uv run scripts/run_experiment.py --run-name v1.1.0-stability-streaming-YYYYMMDD
```

### 4) Validate per-run artifacts

For each eval/results/<run_name>/ confirm:
- index.faiss
- meta.sqlite (WAL checkpointed)
- manifest.json includes git tag/commit/dirty and diagnostics.mode=proxy_entropy
- metrics files exist (metrics.jsonl, summary.json)
- figures/tables directories exist if produced

Also confirm:
- events table is populated in SQLite

### 5) Aggregate reports

Generate a comparison report across the three runs:

```bash
uv run scripts/report.py --run-name v1.1.0-baseline-offline-YYYYMMDD
uv run scripts/report.py --run-name v1.1.0-stability-offline-YYYYMMDD
uv run scripts/report.py --run-name v1.1.0-stability-streaming-YYYYMMDD

uv run scripts/report.py --compare \
  v1.1.0-baseline-offline-YYYYMMDD \
  v1.1.0-stability-offline-YYYYMMDD \
  v1.1.0-stability-streaming-YYYYMMDD
```

If --compare is not implemented, implement the comparison as a non-algorithmic reporting enhancement in a separate prompt cycle; do not do it here.

### 6) Paper-ready artifacts

Write paper artifacts under:
- paper/tables/
- paper/figures/

Minimum outputs:
- A table summarizing drift/churn/overlap/maintenance for each condition
- At least one figure for boundary sensitivity (baseline vs stability)

### 7) Record execution output

Write a concise execution summary to AGENT/<T>-out.md including:
- run names
- key metric deltas
- any failures

Commit and push AGENT files if they changed.

---

## Post-cycle release: v1.2.0 (mandatory)

After all runs and artifacts are produced:
- Update VERSION to 1.2.0
- Add a CHANGELOG.md entry:

```markdown
## v1.2.0
- Run v1.1.0 experiment matrix: baseline vs stability chunking (offline+streaming)
- Generated drift/churn/overlap/maintenance reports and paper artifacts
- AGENT prompt used: `AGENT/<T>-in.md`
```

- Commit via jj, push, tag annotated v1.2.0, push tags.

---

## Stop Condition

Stop when:
- All 3 runs completed with per-run artifacts
- Reports and paper artifacts generated
- AGENT archive committed and pushed
- v1.2.0 is tagged and pushed

Do not change algorithmic code in this prompt.
