# Claude Implementation Prompt — Embedding & Vector Database Experiments

This prompt starts a new implementation thread in the same project. It must be used with the rules in CLAUDE.md and AGENTS.md.

Important: CLAUDE.md must contain the Canonical Prompt Contract (Clean v2) verbatim. If there is any conflict, CLAUDE.md is authoritative and this prompt is subordinate. Those rules are authoritative and override any ambiguity here.

⸻

## Canonical Start Prompt (Authoritative)

Context: Continue the existing CURV Institute project. The PRH-driven Universal Lossless Tokenizer, HHC, and LIL are frozen and treated as prior published work.

Goal: Design, implement, and evaluate stability-driven automatic chunking for vector database embeddings that supports both offline preprocessing and streaming ingestion, and demonstrate improvements in retrieval stability and maintenance cost.

Hypothesis: Chunking based on representational stability diagnostics (curvature, stability margin, neighbor disharmony) produces more stable embedding geometry—lower drift, lower ANN neighbor churn, and higher top-k overlap under reformulation—than heuristic chunking, even though ANN asymptotic complexity remains unchanged.

Constraints: deterministic, reproducible, no semantic changes to the frozen tokenizer/LIL/HHC components.

Deliverables: code (chunker + embedding pipeline), metrics (drift, churn, overlap), paper artifacts (tables, figures, manifests).

Non-goals: Improve ANN asymptotic complexity, chase embedding benchmark accuracy, introduce learning into chunking, or modify frozen components.

⸻

## Context

The prior paper (PRH-driven Universal Lossless Tokenizer + HHC + LIL) is frozen and published. This thread explores a follow-up research direction:

Stability-driven chunking and embedding for vector databases, supporting both offline preprocessing and streaming ingestion.

The goal is not to improve ANN asymptotic complexity or chase benchmark accuracy, but to study embedding stability, drift, and retrieval robustness under updates.

⸻

## Role

Claude acts as a senior ML systems engineer and research collaborator.

Primary posture:
- determinism first,
- metrics before optimization,
- explicit trade-offs,
- minimal scope v0.1.

Parallelize aggressively using subagents.

⸻

## Hypotheses (Explicit)

1. Stability-driven chunking reduces embedding drift under incremental updates.
2. Stability-driven chunking reduces nearest-neighbor churn in ANN indices.
3. Stability-driven chunking improves top-k overlap under query reformulation and domain shift.
4. These benefits apply to both offline and streaming ingestion.

⸻

## Non-Goals

- Do not modify ULT, HHC, or LIL semantics.
- Do not introduce learning into chunking.
- Do not claim asymptotic ANN speedups (log n remains unchanged).
- Do not optimize embedding accuracy benchmarks.

⸻

## Deliverables

### 1. Chunker module
- `chunk_offline(bytes) -> chunks`
- `chunk_stream(iter_bytes) -> iter(chunks)`
- Shared cut-score logic and thresholds
- Deterministic chunk manifests (offsets, diagnostics)

### 2. Embedding pipeline
- Fixed embedding model (version-pinned)
- Identical parameters across conditions
- Baseline chunking vs stability-driven chunking

### 3. Vector engine (authoritative for v0.1)

**FAISS + SQLite pairing**

FAISS responsibilities:
- Store embedding vectors only
- Provide ANN search and neighbor lists
- Deterministic index build (seeded)

SQLite responsibilities (source of truth):
- doc_id, chunk_id, byte offsets
- chunk content hash or pointer
- chunk diagnostics (curvature, stability summaries)
- FAISS internal ID mapping
- embedding checksum + model version
- ingestion run ID / timestamp

SQLite WAL + audit logging (mandatory):
- Enable WAL mode:
  - `PRAGMA journal_mode=WAL;`
  - `PRAGMA foreign_keys=ON;`
  - `PRAGMA synchronous=NORMAL;`
- End-of-run checkpoint:
  - `PRAGMA wal_checkpoint(FULL);`
- Add an explicit events table recording all state changes.

Artifacts per run:
- `index.faiss`
- `meta.sqlite`
- `manifest.json`

### 4. Evaluation suite
- Embedding drift (unchanged content)
- Neighbor churn in ANN graph
- Top-k overlap under query reformulation
- Boundary sensitivity
- Index maintenance cost (re-embeds, rebuilds)

### 5. Paper-ready artifacts
- Tables and figures
- Reproducibility manifests

⸻

## Repository Layout (v0.1)

Implement the follow-up work in a new repo under:
`https://github.com/curv-institute/$DIRNAME`

Recommended tree:

```
$DIRNAME/
  AGENTS.md
  CLAUDE.md
  AGENT/
  .gitignore
  VERSION
  CITATION.cff
  CHANGELOG.md
  ARTIFACT_CHECKLIST.md

  configs/
    default.toml

  scripts/
    make_data.py
    chunk_offline.py
    chunk_stream.py
    embed_index.py
    query_eval.py
    report.py
    run_experiment.py
    reproduce.py
    test_all.py

  src/
    __init__.py
    config.py
    data/
      generator.py
      manifests.py
    chunking/
      cut_score.py
      offline.py
      streaming.py
      manifests.py
    embedding/
      model.py
      vectors.py
    storage/
      sqlite_store.py
      faiss_index.py
      schema.sql
    eval/
      drift.py
      churn.py
      overlap.py
      maintenance.py
      plots.py

  paper/
    main.tex
    references.bib
    figures/
    tables/
    artifact_appendix.tex

  eval/results/
    <run_name>/
      index.faiss
      meta.sqlite
      manifest.json
      metrics.jsonl
      summary.json
      report.md
      figures/
      tables/
      manifests/

  tests/
```

Notes:
- Keep runnable scripts PEP 723 + uv run.
- Use .venv via `uv venv .venv` and ensure `.venv/` is gitignored.
- All long prompts/outputs must be archived under `AGENT/<UNIXTIME>-in.md` and `AGENT/<UNIXTIME>-out.md`.

⸻

## Chunking Algorithm (Guidance)

Chunk boundaries should be chosen based on representational stability signals, not heuristics like fixed token count.

Signals available:
- curvature spikes
- stability margin drops
- HHC neighbor disharmony
- controller intervention thresholds
- explicit LIL structural boundaries (preferred when present)

Offline chunking:
- full-document scan
- choose boundaries at local maxima of instability subject to size constraints

Streaming chunking:
- bounded lookahead
- commit when hard or sustained soft triggers fire
- optional small overlap for recall

⸻

## Metrics (Authoritative)

Primary:
- embedding drift (L2 / cosine change over time)
- ANN neighbor churn (top-k overlap)
- retrieval stability under reformulation

Secondary:
- chunk size distribution
- update locality (fraction of chunks re-embedded per edit)

Never conflate these with LM loss.

⸻

## Evaluation Scenarios

- Incremental append-only updates
- Interleaved edits
- Domain-shifted corpora (text/code/json/logs)
- Query reformulation sets

⸻

## Parallel Subagent Plan (Required)

- Subagent A: chunker implementation + manifests
- Subagent B: embedding + vector DB integration
- Subagent C: evaluation metrics + analysis
- Subagent D: plotting + paper artifacts

⸻

## Success Criteria

- Demonstrable reduction in drift and churn vs baseline chunking
- Stable retrieval under updates
- Clear trade-off articulation (stability vs overhead)

⸻

## Stop Condition

Stop when:
- v0.1 chunker works in offline and streaming modes
- evaluation metrics are computed and plotted
- results are reproducible from manifests
- paper artifacts are generated

Begin by proposing the chunking cut-score and the evaluation metrics, then assign subagents.

⸻

## Chunking Cut-Score (Authoritative v0.1 Proposal)

Chunk boundaries are selected by maximizing a cut-score that estimates representational instability if a chunk is extended. The cut-score is computed on a rolling window during tokenization/chunk assembly using stability diagnostics already available (curvature, stability margin, HHC disharmony) plus optional structural boundaries.

### Definitions

Let position t be the potential boundary between bytes (or between tokens/spans) in a stream.

Compute per-position signals over a short local window (streaming-safe):
- K_t: curvature diagnostic at t (higher = more strain)
- S_t: stability margin at t (higher = more stable)
- D_t: HHC disharmony at t (e.g., Var(||z − z_r||)) (higher = less coherent)
- B_t: structural boundary indicator from LIL or syntax heuristics (0/1)
- L_t: current chunk length in bytes (soft constraint)

Normalize signals within a trailing window (e.g., last 4–16 KB or last N tokens) to make them scale-invariant:
- K̃_t = zscore(K_t) or rank_percentile(K_t)
- S̃_t = zscore(S_t) (used as a penalty)
- D̃_t = zscore(D_t) (0 if HHC disabled)

### Cut-score

Use a bounded, additive score:

```
cut_score(t) =
  wK * relu(K̃_t - k0)
+ wD * relu(D̃_t - d0)
+ wS * relu(s0 - S̃_t)
+ wB * B_t
+ wL * relu((L_t - L_target)/L_target)
```

Where:
- k0, d0, s0 are small thresholds (e.g., 0.5 z-score or 75th percentile equivalents)
- w* are fixed weights (configurable via TOML; defaults conservative)
- wB should dominate when explicit structure exists (LIL role boundaries, JSON object end, etc.)

### Boundary selection policy

**Offline mode:**
- Compute cut_score(t) across the full document.
- Choose boundaries at local maxima of cut_score(t) subject to:
  - min_bytes <= chunk <= max_bytes
  - optional overlap overlap_bytes

**Streaming mode:**
- Maintain a trailing buffer of candidate boundaries over last H bytes (commit horizon).
- Commit when:
  - hard trigger: L_t >= max_bytes, or
  - sustained soft trigger: cut_score(t) above threshold for k steps.
- Commit at the best boundary argmax cut_score(t') within the trailing horizon.

### Required TOML knobs (v0.1)

Add a [chunking] section:
- min_bytes, max_bytes, overlap_bytes, commit_horizon_bytes
- L_target_bytes
- wK, wD, wS, wB, wL
- k0, d0, s0
- use_lil_boundaries = true/false

⸻

## Evaluation Metrics (Authoritative v0.1)

The evaluation must measure stability and maintenance, not semantic accuracy.

### A) Embedding drift (unchanged content)

For identical chunk content across revisions (match by content_sha256):
- drift_cos = 1 - cosine(e_old, e_new)
- drift_l2 = ||e_old - e_new||_2

Report mean/p90 drift and drift distribution.

### B) ANN neighbor churn (index stability)

For a fixed probe set of query vectors (or chunk vectors):
- topk_overlap = |N_k^old ∩ N_k^new| / k
- jaccard_k = |∩| / |∪|
- rank_corr (optional): Kendall tau on ranks for shared neighbors

Report mean/p10 overlap; emphasize boundary regions and post-update windows.

### C) Retrieval stability under query reformulation

For each query family {q_i} that targets the same known subset:
- topk_overlap_across_rewrites
- hit_rate@k against planted anchors (synthetic ground truth)

Report stability deltas baseline vs stability-driven chunking.

### D) Boundary sensitivity

Using known domain-boundary offsets from dataset manifests:
- churn/drift within ±N bytes of boundaries
- compare to interior regions

Report boundary penalty and reduction under stability-driven chunking.

### E) Maintenance cost

- reembed_fraction = (# chunks re-embedded) / (# total chunks)
- index_rebuild_events and total rebuild time (if applicable)
- tombstone_rate and post-filter yield (if tombstones used)

### F) Overhead metrics (secondary)

- chunk size distribution (mean/p90)
- chunk count per MB
- ingestion throughput (MB/s) (secondary; report but do not optimize)

⸻

## Parallel Subagent Work Breakdown (Required)

Assign subagents with strict file/module ownership:

- **Subagent A** (Chunking cut-score + streaming/offline logic):
  - `src/chunking/cut_score.py`, `offline.py`, `streaming.py`, `configs/default.toml [chunking]`
  - emit chunk manifests with boundary scores

- **Subagent B** (Metrics definitions + evaluation harness):
  - `src/eval/drift.py`, `churn.py`, `overlap.py`, `maintenance.py`, `scripts/query_eval.py`

- **Subagent C** (Ground-truth query families + planted anchors):
  - extend `scripts/make_data.py` to emit query sets + anchors + boundary offsets in manifests

- **Subagent D** (Reporting + plots + paper tables/figures):
  - `src/eval/plots.py`, `scripts/report.py`, `paper/tables/`, `paper/figures/`

Each subagent must produce deterministic outputs and update manifests accordingly.
